import requests
from bs4 import BeautifulSoup
import os
import time

# æ•°æ®ä¿å­˜è·¯å¾„
SAVE_DIR = "knowledge_db"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

def fetch_article(url, source_name="æƒå¨åª’ä½“"):
    # ä¼ªè£…æˆæµè§ˆå™¨ï¼ˆå¾ˆé‡è¦ï¼ï¼‰
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    # å¼ºåˆ¶ä¸èµ°ä»£ç†
    proxies = { "http": None, "https": None }
    
    try:
        print(f"ðŸ•¸ï¸ æ­£åœ¨å°è¯•æŠ“å–: {url} ...")
        response = requests.get(url, headers=headers, proxies=proxies, timeout=15)
        
        # --- æ ¸å¿ƒä¿®å¤ï¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œé˜²æ­¢ä¹±ç  ---
        response.encoding = response.apparent_encoding 
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # --- æ ¸å¿ƒä¿®å¤ï¼šæš´åŠ›æå–æ³• ---
        # 1. ç§»é™¤ script å’Œ style æ ‡ç­¾ï¼ˆä¹Ÿå°±æ˜¯ç§»é™¤ä»£ç å’Œæ ·å¼ï¼Œåªç•™æ–‡å­—ï¼‰
        for script in soup(["script", "style"]):
            script.extract()    

        # 2. èŽ·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬
        text = soup.get_text()

        # 3. æŒ‰è¡Œåˆ†å‰²ï¼ŒåŽ»é™¤å¤šä½™ç©ºç™½
        lines = (line.strip() for line in text.splitlines())
        # 4. æŠŠæ¯ä¸€è¡Œé‡æ–°æ‹¼èµ·æ¥ï¼Œåªä¿ç•™é•¿åº¦å¤§äºŽ 50 çš„æ®µè½ï¼ˆè¿‡æ»¤æŽ‰èœå•ã€é¡µè„šï¼‰
        chunks = [phrase.strip() for phrase in lines if len(phrase.strip()) > 50]
        
        full_text = "\n\n".join(chunks)
        
        if len(full_text) < 100:
            print("âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æŠ“å–å¤±è´¥ã€‚")
            return

        # èŽ·å–æ ‡é¢˜
        title = soup.title.string if soup.title else f"æœªçŸ¥æ–‡ç« _{int(time.time())}"
        
        # ä¿å­˜
        filename = f"{source_name}_{int(time.time())}.txt"
        file_path = os.path.join(SAVE_DIR, filename)
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"ã€æ¥æºã€‘ï¼š{source_name}\n")
            f.write(f"ã€é“¾æŽ¥ã€‘ï¼š{url}\n")
            f.write(f"ã€æ ‡é¢˜ã€‘ï¼š{title}\n\n")
            f.write(full_text)
            
        print(f"âœ… æˆåŠŸä¿å­˜ï¼({len(full_text)}å­—)")
        
    except Exception as e:
        print(f"âŒ æŠ“å–å‡ºé”™: {e}")

if __name__ == "__main__":
    # ä½¿ç”¨ä¸€äº›æ›´å®¹æ˜“æŠ“å–çš„é¡µé¢
    urls = [
        ("äººæ°‘ç½‘_æ•°å­—ç»æµŽ", "http://politics.people.com.cn/n1/2024/0101/c1001-40150338.html"),
        ("æ–°åŽç½‘_é«˜è´¨é‡å‘å±•", "http://www.news.cn/politics/20240305/4e3415053426463994a0256860057068/c.html")
    ]
    
    print("=== å¼€å§‹æž„å»ºæƒå¨çŸ¥è¯†åº“ ===")
    for source, link in urls:
        fetch_article(link, source)
        time.sleep(2) 
    
    print("\nðŸŽ‰ ä»»åŠ¡ç»“æŸï¼è¯·é‡å¯ main.py")